"""
Ollama LLM Helper
Integrace s lokÃ¡lnÃ­m Ollama serverem pro:
- AnalÃ½zu obrÃ¡zkÅ¯ obleÄenÃ­ (LLaVA)
- GenerovÃ¡nÃ­ enhanced promptÅ¯
- Styling doporuÄenÃ­
"""

import requests
import base64
from typing import Optional, Dict
from pathlib import Path


class OllamaHelper:
    """Helper pro komunikaci s Ollama serverem"""

    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self.vision_model = "llava"  # nebo "llama3.2-vision"
        self.text_model = "llama3.2"  # nebo jinÃ½ textovÃ½ model

    def is_available(self) -> bool:
        """Check zda Ollama server bÄ›Å¾Ã­"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=2)
            return response.status_code == 200
        except:
            return False

    def list_models(self) -> list:
        """Seznam dostupnÃ½ch modelÅ¯"""
        try:
            response = requests.get(f"{self.base_url}/api/tags")
            if response.status_code == 200:
                return [m["name"] for m in response.json().get("models", [])]
            return []
        except:
            return []

    def analyze_garment(self, image_path: str) -> str:
        """
        Analyzuj obleÄenÃ­ na obrÃ¡zku pomocÃ­ LLaVA

        Args:
            image_path: Cesta k obrÃ¡zku obleÄenÃ­

        Returns:
            DetailnÃ­ textovÃ¡ analÃ½za
        """

        # Load a enkÃ³duj obrÃ¡zek
        with open(image_path, "rb") as f:
            img_bytes = f.read()
            img_b64 = base64.b64encode(img_bytes).decode("utf-8")

        # Prompt pro analÃ½zu
        prompt = """Analyzuj toto obleÄenÃ­ detailnÄ›. OdpovÄ›z strukturovanÄ›:

TYP: (triÄko/koÅ¡ile/mikina/bunda/kalhoty/suknÄ›/Å¡aty/...)
BARVA: (hlavnÃ­ barva a pÅ™Ã­padnÃ© dalÅ¡Ã­ barvy)
VZOR: (jednolitÃ¡/prouÅ¾ky/kostky/potisk/...)
MATERIÃL: (bavlna/denim/kÅ¯Å¾e/fleece/sportovnÃ­/...)
STYL: (casual/formal/sport/vintage/modern/...)
DETAILY: (lÃ­mec/kapuce/knoflÃ­ky/zip/...)

BuÄ struÄnÃ½ ale pÅ™esnÃ½."""

        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.vision_model,
                    "prompt": prompt,
                    "images": [img_b64],
                    "stream": False
                },
                timeout=30
            )

            if response.status_code == 200:
                result = response.json().get("response", "")
                return result.strip()
            else:
                return f"Chyba pÅ™i analÃ½ze: {response.status_code}"

        except Exception as e:
            return f"Chyba: {str(e)}"

    def generate_prompt(
        self,
        garment_analysis: str,
        person_description: str = "osoba",
        style_preference: Optional[str] = None
    ) -> str:
        """
        Generuj optimalizovanÃ½ prompt pro try-on model

        Args:
            garment_analysis: VÃ½stup z analyze_garment()
            person_description: Popis osoby (optional)
            style_preference: PreferovanÃ½ styl (optional)

        Returns:
            Enhanced prompt pro Stable Diffusion / IDM-VTON
        """

        system_prompt = f"""Jsi expert na generovÃ¡nÃ­ promptÅ¯ pro AI image generation.
TvÃ½m Ãºkolem je vytvoÅ™it optimÃ¡lnÃ­ prompt pro virtual try-on model.

INFORMACE:
- Osoba: {person_description}
- ObleÄenÃ­: {garment_analysis}
{f'- Styl preference: {style_preference}' if style_preference else ''}

VytvoÅ™ prompt kterÃ½ zajistÃ­:
1. RealistickÃ© padnutÃ­ obleÄenÃ­ na tÄ›lo
2. SprÃ¡vnÃ© svÄ›tlo a stÃ­ny
3. ZachovÃ¡nÃ­ textury materiÃ¡lu
4. Anatomickou sprÃ¡vnost
5. FotorealistickÃ½ vÃ½sledek

FORMÃT: VraÅ¥ pouze samotnÃ½ prompt, bez vysvÄ›tlenÃ­ nebo komentÃ¡Å™Å¯.
JAZYK: AnglickÃ½ (pro AI modely)
DÃ‰LKA: Max 75 slov

PROMPT:"""

        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.text_model,
                    "prompt": system_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.9
                    }
                },
                timeout=15
            )

            if response.status_code == 200:
                result = response.json().get("response", "").strip()
                # OÄisti od pÅ™Ã­padnÃ½ch ÃºvodnÃ­ch/zÃ¡vÄ›reÄnÃ½ch poznÃ¡mek
                return self._clean_prompt(result)
            else:
                # Fallback
                return self._fallback_prompt(garment_analysis)

        except Exception as e:
            print(f"âš ï¸  Ollama prompt gen chyba: {e}")
            return self._fallback_prompt(garment_analysis)

    def suggest_styling(self, garment_analysis: str) -> Dict[str, str]:
        """
        Navrhni styling tipy pro danÃ© obleÄenÃ­

        Args:
            garment_analysis: AnalÃ½za obleÄenÃ­

        Returns:
            Dict s tipy
        """

        prompt = f"""Na zÃ¡kladÄ› tohoto obleÄenÃ­ poskytni styling rady:

OBLEÄŒENÃ: {garment_analysis}

OdpovÄ›z ve formÃ¡tu:

KOMBINACE: (s ÄÃ­m to dobÅ™e kombinovat)
PÅ˜ÃLEÅ½ITOSTI: (kde/kdy to nosit)
DOPLÅ‡KY: (doporuÄenÃ© doplÅˆky)

BuÄ struÄnÃ½, praktickÃ½, Äesky."""

        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.text_model,
                    "prompt": prompt,
                    "stream": False
                },
                timeout=20
            )

            if response.status_code == 200:
                result = response.json().get("response", "")
                return self._parse_styling_response(result)
            else:
                return {}

        except Exception as e:
            print(f"âš ï¸  Styling suggestions error: {e}")
            return {}

    def _clean_prompt(self, prompt: str) -> str:
        """OÄisti prompt od nepotÅ™ebnÃ½ch ÄÃ¡stÃ­"""
        # OdstranÄ›nÃ­ ÃºvodnÃ­ch frÃ¡zÃ­
        prefixes_to_remove = [
            "prompt:",
            "here's the prompt:",
            "here is the prompt:",
            "the prompt:",
        ]

        prompt_lower = prompt.lower()
        for prefix in prefixes_to_remove:
            if prompt_lower.startswith(prefix):
                prompt = prompt[len(prefix):].strip()

        # OdstranÄ›nÃ­ uvozovek
        prompt = prompt.strip('"\'')

        return prompt

    def _fallback_prompt(self, garment_analysis: str) -> str:
        """Fallback prompt kdyÅ¾ Ollama selÅ¾e"""
        return f"A person wearing {garment_analysis}, photorealistic, high quality, detailed, proper fit, natural lighting, professional photo"

    def _parse_styling_response(self, response: str) -> Dict[str, str]:
        """Parse strukturovanou odpovÄ›Ä styling tipÅ¯"""
        result = {}

        lines = response.split("\n")
        current_key = None

        for line in lines:
            line = line.strip()

            if line.startswith("KOMBINACE:"):
                current_key = "combinations"
                result[current_key] = line.replace("KOMBINACE:", "").strip()
            elif line.startswith("PÅ˜ÃLEÅ½ITOSTI:"):
                current_key = "occasions"
                result[current_key] = line.replace("PÅ˜ÃLEÅ½ITOSTI:", "").strip()
            elif line.startswith("DOPLÅ‡KY:"):
                current_key = "accessories"
                result[current_key] = line.replace("DOPLÅ‡KY:", "").strip()
            elif current_key and line:
                result[current_key] += " " + line

        return result


# CLI test
if __name__ == "__main__":
    import sys

    helper = OllamaHelper()

    print("ğŸ” Testuji Ollama pÅ™ipojenÃ­...")
    if helper.is_available():
        print("âœ… Ollama server bÄ›Å¾Ã­!")
        print(f"ğŸ“¦ DostupnÃ© modely: {', '.join(helper.list_models())}")

        # Test analÃ½zy (pokud je zadÃ¡n obrÃ¡zek)
        if len(sys.argv) > 1:
            image_path = sys.argv[1]
            print(f"\nğŸ–¼ï¸  Analyzuji: {image_path}")

            analysis = helper.analyze_garment(image_path)
            print(f"\nğŸ“‹ ANALÃZA:\n{analysis}")

            prompt = helper.generate_prompt(analysis)
            print(f"\nâœ¨ ENHANCED PROMPT:\n{prompt}")

            styling = helper.suggest_styling(analysis)
            print(f"\nğŸ‘” STYLING TIPY:")
            for key, value in styling.items():
                print(f"  {key}: {value}")

    else:
        print("âŒ Ollama server nenÃ­ dostupnÃ½!")
        print("   SpusÅ¥: ollama serve")
