"""
Try-On Model Wrapper - MAC M4 OPTIMIZED
Podporuje Apple Silicon (MPS) + NVIDIA CUDA + CPU fallback
"""

import torch
from PIL import Image
from typing import Optional
import os
import platform


class TryOnModel:
    """Wrapper pro rÅ¯znÃ© try-on modely s Apple Silicon supportem"""

    def __init__(self, model_type="idm-vton", device="auto"):
        self.model_type = model_type
        self.device = self._get_best_device(device)
        self.pipe = None

        print(f"ğŸ”§ Inicializuji {model_type} model na {self.device}...")
        print(f"ğŸ’» Platform: {platform.system()} {platform.machine()}")
        self._load_model()

    def _get_best_device(self, device: str) -> str:
        """Automaticky detekuj nejlepÅ¡Ã­ device (MPS pro Mac, CUDA pro NVIDIA, CPU fallback)"""

        if device != "auto":
            return device

        # Mac M1/M2/M3/M4 - pouÅ¾ij MPS (Metal Performance Shaders)
        if torch.backends.mps.is_available():
            print("âœ… Apple Silicon (MPS) detekovÃ¡n!")
            return "mps"

        # NVIDIA GPU - pouÅ¾ij CUDA
        elif torch.cuda.is_available():
            print("âœ… NVIDIA CUDA detekovÃ¡n!")
            return "cuda"

        # Fallback na CPU
        else:
            print("âš ï¸  Å½Ã¡dnÃ© GPU, pouÅ¾Ã­vÃ¡m CPU (bude pomalÃ©)")
            return "cpu"

    def _load_model(self):
        """Load pÅ™Ã­sluÅ¡nÃ½ model podle typu"""

        if self.model_type == "idm-vton":
            self._load_idm_vton()
        elif self.model_type == "ootdiffusion":
            self._load_ootdiffusion()
        elif self.model_type == "sd-inpaint":
            self._load_sd_inpaint()
        else:
            raise ValueError(f"NeznÃ¡mÃ½ model type: {self.model_type}")

    def _load_idm_vton(self):
        """Load IDM-VTON model (nejlepÅ¡Ã­ kvalita)"""
        try:
            from diffusers import AutoPipelineForImage2Image

            # Check if model je staÅ¾enÃ½ lokÃ¡lnÄ›
            local_path = "./models/idm-vton"
            if os.path.exists(local_path):
                model_path = local_path
                print(f"âœ… NaÄÃ­tÃ¡m lokÃ¡lnÃ­ model z {local_path}")
            else:
                model_path = "yisol/IDM-VTON"
                print(f"âš ï¸  LokÃ¡lnÃ­ model nenalezen, stahuji z HuggingFace...")
                print(f"   (toto mÅ¯Å¾e trvat nÄ›kolik minut pÅ™i prvnÃ­m spuÅ¡tÄ›nÃ­)")

            # Mac M4: pouÅ¾ij float32 pro MPS, float16 pro CUDA
            dtype = torch.float32 if self.device == "mps" else (torch.float16 if self.device == "cuda" else torch.float32)

            self.pipe = AutoPipelineForImage2Image.from_pretrained(
                model_path,
                torch_dtype=dtype
            ).to(self.device)

            # Optimalizace pro Mac M4
            if self.device == "mps":
                print("ğŸ Optimalizuji pro Apple Silicon...")
                # MPS mÃ¡ problÃ©my s nÄ›kterÃ½mi operacemi, pouÅ¾ij attention slicing
                self.pipe.enable_attention_slicing()
                # Redukuj memory usage
                # self.pipe.enable_vae_slicing()  # NÄ›kdy problematickÃ© na MPS

            print("âœ… IDM-VTON model naÄten")

        except Exception as e:
            print(f"âŒ Chyba pÅ™i naÄÃ­tÃ¡nÃ­ IDM-VTON: {e}")
            print("   ZkouÅ¡Ã­m fallback na SD inpainting...")
            self.model_type = "sd-inpaint"
            self._load_sd_inpaint()

    def _load_ootdiffusion(self):
        """Load OOTDiffusion model"""
        # TODO: Implementovat OOTDiffusion
        # https://github.com/levihsu/OOTDiffusion
        raise NotImplementedError("OOTDiffusion zatÃ­m nenÃ­ implementovÃ¡n")

    def _load_sd_inpaint(self):
        """Fallback: Stable Diffusion Inpainting"""
        try:
            from diffusers import StableDiffusionInpaintPipeline

            dtype = torch.float32 if self.device == "mps" else (torch.float16 if self.device == "cuda" else torch.float32)

            self.pipe = StableDiffusionInpaintPipeline.from_pretrained(
                "runwayml/stable-diffusion-inpainting",
                torch_dtype=dtype
            ).to(self.device)

            # Mac optimalizace
            if self.device == "mps":
                self.pipe.enable_attention_slicing()

            print("âœ… SD Inpainting naÄten (fallback)")

        except Exception as e:
            raise RuntimeError(f"Nelze naÄÃ­st Å¾Ã¡dnÃ½ model: {e}")

    def generate(
        self,
        person_image: Image.Image,
        garment_image: Image.Image,
        prompt: Optional[str] = None,
        num_inference_steps: int = 30,
        guidance_scale: float = 7.5
    ) -> Image.Image:
        """
        Generuj try-on vÃ½sledek

        Args:
            person_image: PIL Image osoby
            garment_image: PIL Image obleÄenÃ­
            prompt: Text prompt (optional, mÅ¯Å¾e bÃ½t enhanced z Ollama)
            num_inference_steps: PoÄet krokÅ¯ (vyÅ¡Å¡Ã­ = lepÅ¡Ã­ kvalita, pomalejÅ¡Ã­)
            guidance_scale: Jak moc dodrÅ¾ovat prompt (7.5 je standard)

        Returns:
            PIL Image s vÃ½sledkem
        """

        # Default prompt
        if prompt is None:
            prompt = "a person wearing the garment, high quality, photorealistic, detailed"

        print(f"ğŸ¨ Generuji s promptem: {prompt}")
        print(f"âš™ï¸  Device: {self.device}, Steps: {num_inference_steps}")

        # Resize pro konzistentnÃ­ velikosti
        # Mac M4: pouÅ¾ij menÅ¡Ã­ size pokud MPS pro rychlejÅ¡Ã­ inference
        max_size = 384 if self.device == "mps" else 512
        person_image = self._resize_image(person_image, max_size=max_size)
        garment_image = self._resize_image(garment_image, max_size=max_size)

        try:
            if self.model_type == "idm-vton":
                # IDM-VTON API
                result = self.pipe(
                    prompt=prompt,
                    image=person_image,
                    garment_image=garment_image,
                    num_inference_steps=num_inference_steps,
                    guidance_scale=guidance_scale
                ).images[0]

            elif self.model_type == "sd-inpaint":
                # SD Inpainting (jednoduÅ¡Å¡Ã­, fallback)
                # PotÅ™ebujeme mask - zde zjednoduÅ¡enÃ© (v production pouÅ¾ij SAM)
                mask = self._create_simple_mask(person_image)

                result = self.pipe(
                    prompt=prompt,
                    image=person_image,
                    mask_image=mask,
                    num_inference_steps=num_inference_steps,
                    guidance_scale=guidance_scale
                ).images[0]

            else:
                raise NotImplementedError(f"Generate nenÃ­ implementovÃ¡no pro {self.model_type}")

        except RuntimeError as e:
            # MPS mÅ¯Å¾e mÃ­t problÃ©my s nÄ›kterÃ½mi operacemi
            if self.device == "mps" and "MPS" in str(e):
                print("âš ï¸  MPS chyba, zkouÅ¡Ã­m s CPU...")
                self.device = "cpu"
                self.pipe = self.pipe.to("cpu")
                return self.generate(person_image, garment_image, prompt, num_inference_steps, guidance_scale)
            else:
                raise e

        return result

    def _resize_image(self, image: Image.Image, max_size: int = 512) -> Image.Image:
        """Resize image keeping aspect ratio"""
        w, h = image.size

        if max(w, h) > max_size:
            if w > h:
                new_w = max_size
                new_h = int(h * (max_size / w))
            else:
                new_h = max_size
                new_w = int(w * (max_size / h))

            image = image.resize((new_w, new_h), Image.Resampling.LANCZOS)

        return image

    def _create_simple_mask(self, image: Image.Image) -> Image.Image:
        """
        VytvoÅ™ jednoduchou masku pro oblast obleÄenÃ­
        V production pouÅ¾ij Segment Anything Model (SAM)
        """
        import numpy as np

        # ZjednoduÅ¡enÃ¡ verze - mask prostÅ™ednÃ­ ÄÃ¡sti tÄ›la
        w, h = image.size
        mask = np.zeros((h, w), dtype=np.uint8)

        # ObdÃ©lnÃ­k pro hornÃ­ ÄÃ¡st tÄ›la (pÅ™ibliÅ¾nÄ›)
        top = int(h * 0.2)
        bottom = int(h * 0.7)
        left = int(w * 0.25)
        right = int(w * 0.75)

        mask[top:bottom, left:right] = 255

        return Image.fromarray(mask)


# Helper pro staÅ¾enÃ­ modelu pÅ™edem
def download_models():
    """Utility pro pÅ™edem staÅ¾enÃ­ modelÅ¯"""
    print("ğŸ“¥ Stahuji modely...")

    from diffusers import AutoPipelineForImage2Image

    # IDM-VTON
    print("ğŸ“¥ IDM-VTON...")
    # Mac M4: pouÅ¾ij float32
    dtype = torch.float32 if torch.backends.mps.is_available() else torch.float16
    pipe = AutoPipelineForImage2Image.from_pretrained(
        "yisol/IDM-VTON",
        torch_dtype=dtype
    )
    pipe.save_pretrained("./models/idm-vton")
    print("âœ… IDM-VTON uloÅ¾en do ./models/idm-vton")

    print("âœ… VÅ¡echny modely staÅ¾eny!")


# Mac M4 specific test
def test_mac_m4():
    """Test funkcionality na Mac M4"""
    print("ğŸ§ª Test pro Mac M4...")
    print(f"PyTorch version: {torch.__version__}")
    print(f"MPS available: {torch.backends.mps.is_available()}")
    print(f"MPS built: {torch.backends.mps.is_built()}")

    if torch.backends.mps.is_available():
        print("âœ… MPS je pÅ™ipraveno!")
        # Test tensor
        x = torch.randn(100, 100).to("mps")
        y = x @ x.T
        print(f"âœ… MPS test tensor operace ÃºspÄ›Å¡nÃ¡!")
        print(f"   Result shape: {y.shape}")
    else:
        print("âŒ MPS nenÃ­ dostupnÃ©")


if __name__ == "__main__":
    # Test nebo download
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "download":
        download_models()
    elif len(sys.argv) > 1 and sys.argv[1] == "test-mac":
        test_mac_m4()
    else:
        # Test run
        print("ğŸ§ª Test try-on model...")
        model = TryOnModel()
        print("âœ… Model inicializovÃ¡n ÃºspÄ›Å¡nÄ›!")
