#!/usr/bin/env python3
"""
Virtual Fitting Room - FastAPI Server
Starter template pro virtuÃ¡lnÃ­ zkouÅ¡ecÃ­ kabinku
"""

from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import FileResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import torch
from PIL import Image
import io
import os
from pathlib import Path
import uuid

# Import custom modules (budou vytvoÅ™enÃ©)
from models.try_on import TryOnModel
from ollama.llm_helper import OllamaHelper

app = FastAPI(
    title="Virtual Fitting Room API",
    description="AI-powered virtual try-on aplikace",
    version="1.0.0"
)

# CORS pro frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # V production nastavit konkrÃ©tnÃ­ domÃ©ny
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global variables
OUTPUT_DIR = Path("./outputs")
OUTPUT_DIR.mkdir(exist_ok=True)

# Initialize models (lazy loading)
try_on_model = None
ollama_helper = None


@app.on_event("startup")
async def startup_event():
    """Initialize models pÅ™i startu serveru"""
    global try_on_model, ollama_helper

    import platform as plat

    print("\n" + "="*60)
    print("ğŸš€ VIRTUAL FITTING ROOM - STARTUP")
    print("="*60)

    # Platform detection
    print("\nğŸ“Š PLATFORM DETECTION:")
    print(f"  OS: {plat.system()} {plat.release()}")
    print(f"  Architecture: {plat.machine()}")
    print(f"  Python: {plat.python_version()}")

    # GPU detection
    print("\nğŸ® GPU DETECTION:")
    if torch.backends.mps.is_available():
        print("  âœ… Apple Silicon (MPS) - Mac M1/M2/M3/M4 detected!")
        print("  ğŸ Optimalizace: float32, 384px, attention slicing")
        device_type = "MPS"
    elif torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
        print(f"  âœ… NVIDIA CUDA - {gpu_name} detected!")
        print(f"  ğŸ® VRAM: {vram_gb:.1f} GB")
        print(f"  ğŸ® Optimalizace: float16, 512px")
        device_type = "CUDA"
    else:
        print("  âš ï¸  Å½Ã¡dnÃ© GPU - pouÅ¾Ã­vÃ¡m CPU (bude VELMI pomalÃ©!)")
        print("  ğŸ’» DoporuÄuji Mac M1+ nebo NVIDIA GPU")
        device_type = "CPU"

    print(f"\n{'='*60}")
    print(f"ğŸ”§ LOADING MODELS (device={device_type})...")
    print(f"{'='*60}\n")

    # Try-on model (s auto-detekcÃ­ device)
    try:
        try_on_model = TryOnModel(device="auto")  # automaticky vybere MPS/CUDA/CPU
        print("\nâœ… Try-on model naÄten ÃºspÄ›Å¡nÄ›!")
    except Exception as e:
        print(f"\nâŒ Try-on model CHYBA: {e}")
        print("   Server bude omezenÃ½!")

    # Ollama helper
    print("\nğŸ¤– OLLAMA DETECTION:")
    try:
        ollama_helper = OllamaHelper()
        if ollama_helper.is_available():
            models = ollama_helper.list_models()
            print(f"  âœ… Ollama server pÅ™ipojen!")
            print(f"  ğŸ“¦ DostupnÃ© modely: {', '.join(models[:3])}...")
        else:
            print("  âš ï¸  Ollama server nedostupnÃ½")
            print("  ğŸ’¡ SpusÅ¥: ollama serve")
    except Exception as e:
        print(f"  âš ï¸  Ollama chyba: {e}")

    print("\n" + "="*60)
    print("âœ… SERVER READY!")
    print("="*60)
    print(f"ğŸŒ API: http://localhost:8000")
    print(f"ğŸ“š Docs: http://localhost:8000/docs")
    print(f"ğŸ¨ Frontend: http://localhost:3000 (pokud bÄ›Å¾Ã­)")
    print("="*60 + "\n")


@app.get("/")
async def root():
    """Health check endpoint with platform info"""
    import platform as plat

    # Detect device
    if torch.backends.mps.is_available():
        device = "mps"
        device_name = "Apple Silicon (MPS)"
    elif torch.cuda.is_available():
        device = "cuda"
        device_name = torch.cuda.get_device_name(0)
    else:
        device = "cpu"
        device_name = "CPU"

    return {
        "status": "running",
        "service": "Virtual Fitting Room API",
        "version": "1.0.0",
        "platform": {
            "os": plat.system(),
            "architecture": plat.machine(),
            "python": plat.python_version(),
            "device": device,
            "device_name": device_name
        },
        "models": {
            "try_on": try_on_model is not None,
            "ollama": ollama_helper is not None and ollama_helper.is_available()
        }
    }


@app.get("/health")
async def health():
    """Simple health check endpoint"""
    return {
        "status": "OK",
        "models_loaded": try_on_model is not None
    }


@app.post("/api/tryon")
async def try_on(
    person_image: UploadFile = File(..., description="Fotografie osoby (celÃ© tÄ›lo)"),
    garment_image: UploadFile = File(..., description="Fotografie obleÄenÃ­"),
    use_ollama: bool = True
):
    """
    HlavnÃ­ endpoint pro virtual try-on

    Args:
        person_image: JPG/PNG fotografie osoby
        garment_image: JPG/PNG fotografie obleÄenÃ­
        use_ollama: PouÅ¾Ã­t Ollama pro prompt enhancement

    Returns:
        JSON s URL vÃ½slednÃ©ho obrÃ¡zku
    """

    if not try_on_model:
        raise HTTPException(status_code=503, detail="Try-on model nenÃ­ naÄten")

    try:
        # Load images
        person_img = Image.open(io.BytesIO(await person_image.read())).convert("RGB")
        garment_img = Image.open(io.BytesIO(await garment_image.read())).convert("RGB")

        # Ollama enhancement (optional)
        enhanced_prompt = None
        garment_analysis = None

        if use_ollama and ollama_helper and ollama_helper.is_available():
            print("ğŸ¤– Analyzuji obleÄenÃ­ pomocÃ­ Ollama...")

            # UloÅ¾ doÄasnÄ› garment pro analÃ½zu
            temp_garment_path = OUTPUT_DIR / f"temp_{uuid.uuid4()}.jpg"
            garment_img.save(temp_garment_path)

            # AnalÃ½za obleÄenÃ­
            garment_analysis = ollama_helper.analyze_garment(str(temp_garment_path))

            # Generuj enhanced prompt
            enhanced_prompt = ollama_helper.generate_prompt(
                garment_analysis,
                "osoba na fotce"
            )

            # Cleanup temp
            temp_garment_path.unlink()

            print(f"ğŸ“ Enhanced prompt: {enhanced_prompt}")

        # Run try-on
        print("ğŸ¨ Generuji try-on vÃ½sledek...")
        result_image = try_on_model.generate(
            person_img,
            garment_img,
            prompt=enhanced_prompt
        )

        # Save result
        result_filename = f"result_{uuid.uuid4()}.jpg"
        result_path = OUTPUT_DIR / result_filename
        result_image.save(result_path, quality=95)

        return JSONResponse({
            "success": True,
            "result_url": f"/outputs/{result_filename}",
            "garment_analysis": garment_analysis,
            "enhanced_prompt": enhanced_prompt
        })

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Chyba pÅ™i zpracovÃ¡nÃ­: {str(e)}")


@app.post("/api/tryon/multiview")
async def try_on_multiview(
    person_image: UploadFile = File(...),
    garment_image: UploadFile = File(...),
    num_views: int = 8
):
    """
    Generuj multi-view vÃ½sledky pro 3D rotaci

    Args:
        person_image: Fotografie osoby
        garment_image: Fotografie obleÄenÃ­
        num_views: PoÄet pohledÅ¯ (doporuÄeno 8)

    Returns:
        JSON s URLs vÅ¡ech pohledÅ¯
    """

    # TODO: Implementovat multi-view generaci
    raise HTTPException(status_code=501, detail="Multi-view zatÃ­m nenÃ­ implementovÃ¡n")


@app.post("/api/analyze-garment")
async def analyze_garment(
    garment_image: UploadFile = File(...)
):
    """
    Analyzuj obleÄenÃ­ pomocÃ­ Ollama LLaVA

    Returns:
        JSON s analÃ½zou (typ, barva, materiÃ¡l, styl)
    """

    if not ollama_helper or not ollama_helper.is_available():
        raise HTTPException(status_code=503, detail="Ollama nenÃ­ dostupnÃ½")

    try:
        # Save temp
        garment_img = Image.open(io.BytesIO(await garment_image.read()))
        temp_path = OUTPUT_DIR / f"temp_{uuid.uuid4()}.jpg"
        garment_img.save(temp_path)

        # Analyze
        analysis = ollama_helper.analyze_garment(str(temp_path))

        # Optional: styling tips
        # styling = ollama_helper.suggest_styling("...")

        # Cleanup
        temp_path.unlink()

        return JSONResponse({
            "success": True,
            "analysis": analysis
        })

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/outputs/{filename}")
async def get_output(filename: str):
    """Serve vÃ½slednÃ© obrÃ¡zky"""
    file_path = OUTPUT_DIR / filename
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="Soubor nenalezen")
    return FileResponse(file_path)


if __name__ == "__main__":
    import uvicorn

    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘   Virtual Fitting Room API Server            â•‘
    â•‘   ğŸ¨ AI-Powered Virtual Try-On                â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        reload=True  # Hot reload pro development
    )
